{
  "hash": "75213bf1a9fe2cd7af621106532f030d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Mini-Project 4: Bayesian Analysis\"\nformat: html\n---\n\n\n\n\n\nIn this project we will be looking at the probability that Rafael Nadal wins a point on his serve against Novak Djokovic on the Clay surface. We will use 3, priors, 1 non-informative, 2 informative, and data from the players most recent match to create 3 different posterior distributions. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n:::\n\n\n\n\n\n## Non-informative Prior\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nps <- seq(0, 1, length.out = 1000)\n\nnoninformative_alpha <- 1\nnoninformative_beta <- 1\n\nnoninformative_prior <- dbeta(ps,\n                              noninformative_alpha, noninformative_beta)\n```\n:::\n\n\n\n\n\n## First Informative Prior\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Nadal won 46/66 points \n## phat = 46/66 = 0.697\n## std error = 0.05657\n## 0.697 = alpha/(alpha + beta)\n## 0.697(a) + 0.697(b) = a\n## 0.697(b) = 0.303(a)\n## beta = (0.303(alpha))/0.697\n\ntarget_mean <- 0.697\n\nalphas <- seq(0.1, 60, length.out = 500)\nbetas <- (0.303*alphas)/0.697\n\n\nparam_df <- tibble(alphas, betas)\nparam_df <- param_df |> mutate(vars = \n                    (alphas*betas)/((alphas + betas)^2*(alphas + betas + 1)))\n\n\ntarget_var <- 0.05657 ^ 2\n\nparam_df <- param_df |> mutate(dist_to_target = abs(vars - target_var))\n\nparam_df |> filter(dist_to_target == min(dist_to_target))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 4\n  alphas betas    vars dist_to_target\n   <dbl> <dbl>   <dbl>          <dbl>\n1   45.4  19.7 0.00320     0.00000378\n```\n\n\n:::\n\n```{.r .cell-code}\nalpha <- 45.4\n\nbeta <- 19.7\n\ninformative_prior_1 <- dbeta(ps, alpha, beta)\n```\n:::\n\n\n\nFor this informative prior, I set out to find an alpha and beta that gave a target mean of 0.697, which matched Nadal's win rate, and a variance of 0.05657^2 that matched the standard deviation. I do find the best fit for alpha and beta, I found the relationship between alpha and beta such that the mean is equal to 0.697, and then set the target variance to the square of the standard deviation. I then found the combination of alpha and beta that had the variance closest to the target. Our assumptions were the mean and std dev that were found with the past match data.\n\n\n\n## Second Informative Prior\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## trying to get a mean of 0.75 and a probability\n## that lambda is less than 0.7 equal to 0.02\nalphas <- seq(0.01, 300, length.out = 2000) \nbetas <- (0.25*alphas)/0.75\n\ntarget_prob <- 0.02\nprob_less_0.7 <- pbeta(0.7, alphas, betas)\n\ntibble(alphas, betas, prob_less_0.7) |>\n  mutate(close_to_target = abs(prob_less_0.7 - target_prob)) |>\n  filter(close_to_target == min(close_to_target))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 4\n  alphas betas prob_less_0.7 close_to_target\n   <dbl> <dbl>         <dbl>           <dbl>\n1   252.  83.9        0.0200      0.00000517\n```\n\n\n:::\n\n```{.r .cell-code}\nalpha2 <- 251.828\n\nbeta2 <- 83.943\n\ninformative_prior_2 <- dbeta(ps, alpha2, beta2)\n```\n:::\n\n\n\nFor this informative prior, I was looking for a mean of 0.75, and was trying to find a distribution that had nearly all of the data above 0.7. For this, I set a target that only 2% of the data would be below 0.7, and iterated through alpha and beta to find the the values where only 2% of data was below 0.7. Beta was also found to be proportional to alpha, where betas = (0.25*alphas)/0.75. Our assumptions were that the mean is 0.75, and that the majority of the data, 98 percent, is greater than 0.7.\n\n\n\n## Prior Distribution Plot\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprior_plot <- tibble(ps, informative_prior_2, informative_prior_1, noninformative_prior) |>\n  pivot_longer(2:4, names_to = \"prior_type\", values_to = \"density\")\n\nggplot(data = prior_plot, aes(x = ps, y = density, colour = prior_type)) +\n  geom_line() +\n  scale_colour_viridis_d(end = 0.9) +\n  theme_minimal() +\n  labs(x = \"p\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Non-Informative Posterior\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## 56 wins on 84 serves\n## alpha_post = yobs + alpha = 56 + 1 = 57\n## beta_post = n - yobs + beta = 84 - 56 + 1 = 29\nnon_inf_alpha_post <- 57\nnon_inf_beta_post <- 29\n\nnoninformative_post <- dbeta(ps,\n                              non_inf_alpha_post, non_inf_beta_post)\n```\n:::\n\n\n\nFor the non informative posterior, we use the equations alpha_post = yobs + alpha, and beta_post = n - yobs + beta, to get our variables for the posterior distribution.\n\n## First Informative Posterior\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## 56 wins on 84 serves\n## alpha_post = yobs + alpha = 56 + 45.4 = 101.4\n## beta_post = n - yobs + beta = 84 - 56 + 19.7 = 47.7\n\ninf_alpha_post1 <- 101.4\ninf_beta_post1 <- 47.7\n\n\ninformative_post_1 <- dbeta(ps,\n                              inf_alpha_post1, inf_beta_post1)\n```\n:::\n\n\n\n\n## Second Informative Posterior\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## 56 wins on 84 serves\n## alpha_post = yobs + alpha = 56 + 251.828 = 307.828\n## beta_post = n - yobs + beta = 84 - 56 + 83.943 = 111.943\n\ninf_alpha_post2 <- 307.828\ninf_beta_post2 <- 111.942\n\n\ninformative_post_2 <- dbeta(ps,\n                              inf_alpha_post2, inf_beta_post2)\n```\n:::\n\n\n\nFor the informative posteriors, we also use the equations alpha_post = yobs + alpha, and beta_post = n - yobs + beta, to get our variables for the posterior distributions. The beta and alpha are different for each of these and the non-informative prior.\n\n## Posterior Distribution Plot\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npost_plot <- tibble(ps, informative_post_2, informative_post_1, noninformative_post) |>\n  pivot_longer(2:4, names_to = \"posterior_type\", values_to = \"density\")\n\nggplot(data = post_plot, aes(x = ps, y = density, colour = posterior_type)) +\n  geom_line() +\n  scale_colour_viridis_d(end = 0.9) +\n  theme_minimal() +\n  labs(x = \"p\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Posterior Means and 90% Credible Intervals\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnon_inf_mean <- non_inf_alpha_post/(non_inf_alpha_post + non_inf_beta_post)\nnon_inf_upper <- qbeta(0.95, non_inf_alpha_post, non_inf_beta_post)\nnon_inf_lower <- qbeta(0.05, non_inf_alpha_post, non_inf_beta_post)\nnon_inf <- tibble(lower = non_inf_lower,\n                  mean = non_inf_mean,\n                  upper = non_inf_upper)\nnon_inf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  lower  mean upper\n  <dbl> <dbl> <dbl>\n1 0.577 0.663 0.744\n```\n\n\n:::\n\n```{.r .cell-code}\ninf_mean_1 <- inf_alpha_post1/(inf_alpha_post1 + inf_beta_post1)\ninf_lower_1 <- qbeta(0.05, inf_alpha_post1, inf_beta_post1)\ninf_upper_1 <- qbeta(0.95, inf_alpha_post1, inf_beta_post1)\ninf_1 <- tibble(lower = inf_lower_1,\n                mean = inf_mean_1,\n                upper = inf_upper_1)\ninf_1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  lower  mean upper\n  <dbl> <dbl> <dbl>\n1 0.616 0.680 0.741\n```\n\n\n:::\n\n```{.r .cell-code}\ninf_mean_2 <- inf_alpha_post2/(inf_alpha_post2 + inf_beta_post2)\ninf_lower_2 <- qbeta(0.05, inf_alpha_post2, inf_beta_post2)\ninf_upper_2 <- qbeta(0.95, inf_alpha_post2, inf_beta_post2)\ninf_2 <- tibble(lower = inf_lower_2,\n                mean = inf_mean_2,\n                upper = inf_upper_2)\ninf_2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  lower  mean upper\n  <dbl> <dbl> <dbl>\n1 0.697 0.733 0.768\n```\n\n\n:::\n:::\n\n\n\n\nFor each posterior mean, we use the equation, alpha/(alpha + beta) to find the mean. For the 90% credible intervals, we are looking for the bounds that give us the middle 90%, so using qbeta(p_prop, alpha, beta) gives us the desired bounds with the p_props of 0.05 and 0.95. Comparing the 3 posteriors, they all have slightly different ranges, with the non-informative having a mean of 0.663, and a 90% credible interval of (0.577, 0.744). This means for the non-informative there is a 90% probability that the proportion of clay court points Nadal wins against Djokovic is between 0.577 and 0.744. The 2 informative priors are different, with higher means and less variability. The first informative posterior, the mean is 0.68, and the 90% credible interval is (0.616, 0.741). The interval is much smaller than the non-informative. The second informative posterior has a slightly higher mean of 0.733, and a narrower 90% credible interval from (0.697, 0.768). The posteriors are all different because they have different data making the priors. The first prior has a wider credible interval and lower mean because there every p had the same probability. The informative posterior credible intervals were narrower because they had a prior distribution with means that were closer to the sample proportion. The 2 informative posteriors are different as they were designed with different data, and had different means and standard deviations based on their design parameters. The narrowest interval, the second informative posterior, is the narrowest because it was the most confident in its mean, and had the lowest standard deviation. I would select the first informative as the best. The first informative has a narrower 90% credible interval than the non-informative, and has a mean that is only slightly higher than our observed data. The second informative has a smaller credible interval, but the observed data is not in the credible interval. The beta and alpha from this prior were very large, so the data did not have as much of an impact on the posterior. The non-informative posterior is only based on the observed data so it has the same mean, but is less confident so has a much wider credible interval. The first informative uses data from a previous Nadal-Djokovic match so the posterior is a balance of the two matches and creates a distribution that is a good approximation for the proportion of points Nadal wins. \n\n## Conclusion\nIn this mini project we looked at the proportion of points that Nadal won on his serve in clay court matches against Djokovic. We made 3 prior distributions and then used the data from their match at the 2020 French Open to make 3 posterior distributions. We saw the different effect that the deviation of the prior distribution has on the posterior distribution. A smaller deviation means that alpha and beta values are larger, which means that our data has less of an effect on the posterior distribution. Our second prior has very large alpha and beta values, so even with the observed data our posterior distribution credible interval does not cover the observed sample proportion. For the non-informative prior the credible interval is significantly wider, and the standard deviation is large. This project shows that choosing an appropriate prior is very important for attaining a good posterior, as having a highly weighted prior that does not agree with the sample data will result in a posterior that does not include the observed data in the credible interval. Choosing a prior with a reasonable mean and medium standard deviation is probably most appropriate for finding a posterior that is appropriate for the scenario.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}