[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "math_stat_portfolio",
    "section": "",
    "text": "Mini-Project 1: Sampling Distribution of the Sample Minimum and Maximum\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMini-Project 2: A Meaningful Story\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMini-Project 3: Simulation to Investigate Confidence Intervals\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMini-Project 4: Bayesian Analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMini-Project 5: Advantages and Drawbacks of Using p-values\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nApr 28, 2025\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nApr 25, 2025\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog is a reflection of the 5 Math/Stat Mini-Projects, and a reflection of the connections between the projects.\nThe first mini-project looks at the sampling distribution of sample statistics besides the sample mean, in this case the sample minimum and maximum."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/01_simulation/index.html",
    "href": "posts/01_simulation/index.html",
    "title": "index",
    "section": "",
    "text": "Here is my mini-project\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nggplot(mtcars, aes(x = mpg)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "posts/01_sample_stats/index.html",
    "href": "posts/01_sample_stats/index.html",
    "title": "Mini-Project 1: Sampling Distribution of the Sample Minimum and Maximum",
    "section": "",
    "text": "library(tidyverse)\n\n\n\n\n\n\n\n\n\n\nMinimum Sampling Distribution Plots\n\nnorm_min\n\n\n\n\n\n\n\nunif_min\n\n\n\n\n\n\n\nexp_min\n\n\n\n\n\n\n\nbeta_min\n\n\n\n\n\n\n\n\nMaximum Sampling Distribution Plots\n\nnorm_max\n\n\n\n\n\n\n\nunif_max\n\n\n\n\n\n\n\nexp_max\n\n\n\n\n\n\n\nbeta_max\n\n\n\n\n\n\n\n\n\nTable of Results\n\n\n\n\n\n\n\n\n\n\n\\(\\text{N}(\\mu = 10, \\sigma^2 = 4)\\)\n\\(\\text{Unif}(\\theta_1 = 7, \\theta_2 = 13)\\)\n\\(\\text{Exp}(\\lambda = 0.5)\\)\n\\(\\text{Beta}(\\alpha = 8, \\beta = 2)\\)\n\n\n\n\n\\(\\text{E}(Y_{min})\\)\n7.67\n8.01\n0.387\n0.645\n\n\n\\(\\text{E}(Y_{max})\\)\n12.3\n12.0\n4.52\n0.921\n\n\n\n\n\n\n\n\n\n\\(\\text{SE}(Y_{min})\\)\n1.31\n0.835\n0.388\n0.108\n\n\n\\(\\text{SE}(Y_{max})\\)\n1.34\n0.849\n2.37\n0.0457\n\n\n\n\nBriefly summarise how SE(Ymin) and SE(Ymax) compare for each of the above population models. Can you propose a general rule or result for how SE(Ymin) and SE(Ymax)compare for a given population?\n\nFor the normal distribution, we see that the Standard Error is essentially the same for both the sample min and max. This makes sense as a normal distribution is symmetrical, so the min and max have an equal probability to be the same distance from the sample mean. The variation in the min and max should also be the same because the variation in the distribution is the same above and below the center. The uniform distribution also has the same Standard Error for the sample min and max. This also makes sense as the uniform distribution has an equal probability for each value and the variation in mins and maxes should be the same. The exponential distribution and Beta distribution do not have the same Standard Error for the min and max. For the exponential distribution, this can be explained by smaller values having a higher density, therefore allowing less variation in the sample minimum than the sample maximum. The higher density at small values means that the minimum with be near the minimum allowed value every time leading to a low variation, while there is much more variation in the maximum. For the Beta distribution, the values on the higher end have a much higher density, leading to less variation in the sample maximum than the sample minimum. The sample minimum has much more variation, as the low density leads to minimums that are more spread. We can predict that for uniform and normal distributions, the standard error for the mins and maxes will be the same, while for exponential distributions, the sample minimum will have a smaller standard error, and the beta distribution will have a smaller standard error for the sample maximum.\n\nChoose either the third (Exponential) or fourth (Beta) population model from the table above. For that population model, find the pdf of ymin and ymax, and, for each of those random variables, sketch the pdfs and use integration to calculate the expected value and standard error. What do you notice about how your answers compare to the simulated answers? Some code is given below to help you plot the pdfs in R:\n\nYmin:\npdf(Ymin): n(1-cdf(Y))^n-1 * pdf(Y)\nExpected Value: 0.4\nStandard Error: sqrt(0.32 - 0.4^2) = sqrt(0.16) = 0.4\nYmax:\npdf(Ymax): n(cdf(Y))^n-1 * pdf(Y)\nExpected value = 4.566\nStandard Error = sqrt(26.709 - 4.566^2) = sqrt(5.861) = 2.421\nThe calculated answers are close to the simulated answers, with all of them being slightly higher than the simulated values. Overall the simulated values are all very close, within around 0.1 to 0.2 units from the calculated values.\n\nn &lt;- 5\nlambda &lt;- 0.5\n## CHANGE 0 and 3 to represent where you want your graph to start and end\n## on the x-axis\nx &lt;- seq(0, 6, length.out = 1000)\n## CHANGE to be the pdf you calculated. Note that, as of now, \n## this is not a proper density (it does not integrate to 1).\ndensity &lt;-n*lambda*exp(-n*lambda*x)\n\n\n## put into tibble and plot\nsamp_min_df &lt;- tibble(x, density)\nggplot(data = samp_min_df, aes(x = x, y = density)) +\n  geom_line() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nn &lt;- 5\nlambda &lt;- 0.5\n## CHANGE 0 and 3 to represent where you want your graph to start and end\n## on the x-axis\nx &lt;- seq(0, 15, length.out = 1000)\n## CHANGE to be the pdf you calculated. Note that, as of now, \n## this is not a proper density (it does not integrate to 1).\ndensity &lt;- n*lambda*exp(-lambda*x)*(1-exp(-lambda*x))^(n-1)\n\n\n## put into tibble and plot\nsamp_max_df &lt;- tibble(x, density)\nggplot(data = samp_max_df, aes(x = x, y = density)) +\n  geom_line() +\n  theme_minimal()"
  },
  {
    "objectID": "posts/02_meaningful_story/index.html",
    "href": "posts/02_meaningful_story/index.html",
    "title": "Mini-Project 2: A Meaningful Story",
    "section": "",
    "text": "It was a sunny Monday afternoon in Germany where two old men were sitting outside a pub, chatting away about their days.\n“What’s the likelihood a guy can get a beer around here?”, one joked.\n“I think its pretty high” replied the other, “A lot better than at that café over there for sure”.\n“That’s probably true” conceded the first, “but if you were to give an estimate what would you say?”\n“Only about 25%, but that’s because the pub doesn’t open for another 40 minutes”, explained the second man.\n“That logic seems pretty flawed; what type of estimator would you use to get such a high likelihood” questioned the first.\n“You own the pub” exclaimed the second man, “I simply estimated the odds you would find the key in the next few minutes to be about 0.25.”\n“Well, I guess I should start looking” replied the first, “I do only hide the key in 4 different spots, where should I start?”\n“Just pick one randomly.”\nAcross the street, a schoolteacher and students were watching this exchange, and discussing the situation the men found themselves in.\n“Now children, what probability distribution does this situation follow?” asked the teacher. None of them responded.\n“It’s a binomial distribution” explained the teacher, “there is a success and failure and set number of trials.”\n“No its not” piped up a student, “It’s a geometric distribution, the man only looks until he finds the key.”\n“But what about sampling without replacement?” questioned the teacher.\n“I’ve watched him look for his keys before” replied the student, “his commitment to a random search is astonishing, he looks in the same spot repeatedly if it is randomly selected.”\n“I guess the search does function as a Geometric Random Variable” conceded the teacher, “with each time he searches giving us a random sample from the distribution.”\n“But what are the parameters for the distribution?” queried another student, “what would I put into R-studio to look at random samples from the distribution.”\n“This Geometric distribution would have the parameter of p equals 0.25” answered the teacher, “but why are you so interested in when the pub opens?”\nBack across the street the old men were looking back at the teacher and students and were commenting on how they looked.\n“The way they are standing looks similar to a normal distribution” said the pub owner, “the tallest are in the middle, with the shortest on the ends.”\n“It looks like the tallest are slightly to the right of where the mean would be” joked the second, “looks like your distribution might have a little Bias.”\n“Maybe it does”\n“The variance looks a little weird also” continued the second, “the people on the ends aren’t short enough which would indicate that the distribution has high variance.”\n“I guess so”\n“What’s the matter with you?”\n“I just wish I could get in the pub.”\n“You own the pub you Nimrod.”\n“Oh yeah, time to start looking.”\nBack across the street the students watched attentively as the owner began to search for his keys.\n“He is not very consistent at finding his keys” a student remarked, “maybe he would have more luck if he hid them in one spot every time.”\n“To be consistent he would have to change the way he hides his keys” echoed the teacher, “he could put them in the same place every time as to be able to find the keys on the first try each time.”\n“He could just switch to 2 spots as well” countered another student.\n“That wouldn’t be consistent though” answered another, “he wouldn’t find them in a consistent amount of time with his random search methods.”\nAcross the street the owner found the keys after 15 searches, due to the high variability in his search methods. His pub opens at a drastically different time each morning due to his idiotic search methods, due to his crazy obsession with statistical randomness. The children were not allowed to go to the pub. The End."
  },
  {
    "objectID": "posts/05_using_p_values/index.html",
    "href": "posts/05_using_p_values/index.html",
    "title": "Mini-Project 5: Advantages and Drawbacks of Using p-values",
    "section": "",
    "text": "Questions: 1. Towards the end of Section 1, the authors say “As ‘statistical significance’ is used less, statistical thinking will be used more.” Elaborate on what you think the authors mean. Give some examples of what you think embodies “statistical thinking.”\nI think the authors mean that as we move away from significant and not-significant p-values being strictly defined, there will be more room for exploration into what the p-value really represents. For projects that have p-values that are not far apart but one is significant and the other is not, they will have more in common despite results that are characterized as different. Statistical thinking takes into consideration values that are not significant according to p &lt; 0.05, but that clearly have some difference that has been calculated. There is more ways to interpret the p-value than just significance level, and statistical thinking looks not at the final result of the p-value, but the differences in the test.\n\nSection 2, third paragraph: The authors state “A label of statistical significance adds nothing to what is already conveyed by the value of p; in fact, this dichotomization of p-values makes matters worse.” Elaborate on what you think the authors mean.\n\nI think the authors mean that making p-values have a strictly defined line of what is significant or not doesn’t add anything to how the p-value communicates the results of the tests. You could have 2 values that are 0.002 apart that give different levels of significance, which is not useful in the interpretation of the data. These p-values being interpreted differently is really just inaccurate, as they are very similar. Having a p-value 0.03 away from the level of significance is also not necessarily an indicator that there is no significant difference in the result. Having dichotomous p-values causes us to lose potentially interesting conclusions and findings because we define 2 similar values as strictly apart of one category.\n\nSection 2, end of first column: The authors state “For the integrity of scientific publishing and research dissemination, therefore, whether a p-value passes any arbitrary threshold should not be considered at all when deciding which results to present or highlight.” Do you agree or disagree? How should it be decided which results to present/highlight in scientific publishing?\n\nI agree, using whether a test is significant to determine what results should be given publicity is not fair. Results should be decided based on the quality of the study and not if the results reached the significance level denoted by the p-value. Publishers/organizers should take into account the potential impact of the study and quality of the research over the final p-value result. If significant results are what causes studies to be given publicity, researchers could change the goals for their research from creating interesting and quality studies to finding ways to make statistical tests significant. This could develop into an issue, especially if researchers are tweaking their data/methods to appease the statistical whims of a p-value. The p-value and its significance should not be a deciding factor on what is done with research studies due to the arbitrary threshold it does or does not reach.\n\nSection 3, end of page 2: The authors state “The statistical community has not yet converged on a simple paradigm for the use of statistical inference in scientific research – and in fact it may never do so. A one-size-fits-all approach to statistical inference is an inappropriate expectation.” Do you agree or disagree? Explain.\n\nI agree that there is more than one way to look for statistical significance in the results of a test. Different p-values can represent similar results, and looking at those values differently is not very fair. Different tests have different parameters, and comparing those levels of significance as the same isn’t necessarily an accurate measure of the differences between the data. Different research has different ways of defining differences, and each case has its own measure or level of success. Evaluating each case on its own allows us to assess the research as significant or not according to that specific instance. Comparing very different types of research using a single statistical value is not necessarily an accurate way to understand the relationships in the data.\n\nSection 3.2: The authors note that they are envisioning “a sort of ‘statistical thoughtfulness’.” What do you think “statistical thoughtfulness” means? What are some ways to demonstrate “statistical thoughtfulness” in an analysis?\n\nStatistical thoughtfulness means including a variety of different statistical tests and viewing the data in different ways to fully consider how the data might be significant or not. The thoughtfulness is expanding on how the data is viewed beyond a simple p-value test, and finding more ways to test the significance of the findings. Some ways to demonstrate statistical thoughtfulness in an analysis is to use more statistical techniques like bootstrapping a difference in means to see any differences in the data. Looking at data in a few different ways is useful for gaining a deeper understanding of the research and whether or not the data can be further studied. Looking at data and finding a p-value is not the best way to really understand the results the data is producing, it is a simple look into what it has to offer.\n\nSection 3.2.4: A few of the authors of papers in this special issue argue that some of the terminology used in statistics, such as “significance” and “confidence” can be misleading, and they propose the use of “compatibility” instead. What you do you think they believe the problem is? Do you agree or disagree (that there is a problem and that changing the name will help)?\n\nThey believe that the terms “significance” and “confidence” are misleading and can create false trust in the results of the test. They think that seeing intervals and p-values creates a trust in the results that are not necessarily correct, and believe that compatibility better communicates the relationship between the null hypothesis and the significance of the data. Compatibility interval also better communicates the purpose of the confidence interval, and would be easier to understand for scientific researchers. I agree that there is a problem, but I think that changing the terminology would make people more confused as so many people use the current terminology. Transforming the words significance and confidence to compatibility would be difficult to do, especially with the widespread uses. They do not clear up enough confusion to be worth the effort.\n\nFind a quote or point that really strikes you (i.e., made you think). What is the quote (and tell me where to find it), and why does it stand out to you? My favorite quote comes at the bottom of page 5. All researchers, irrespective of their philosophy or practice, use expert judgment in developing models and interpreting results,” say Brownstein et al. “We must accept that there is subjectivity in every stage of scientific inquiry, but objectivity is nevertheless the funda- mental goal. Therefore, we should base judgments on evidence and careful reasoning, and seek wherever possible to eliminate potential sources of bias.”\n\nThis quote stood out to me as it connected ideas in scientific research to statistical analysis. It connects the scientific idea that you need to look at your results and research and make changes based on observations. It does however note the need to eliminate bias, and have some sort of evidence and careful reasoning as a basis for judgement. This connects to the idea of interpreting p-values based on circumstances and research, and not just whether they achieve a significant p-value."
  },
  {
    "objectID": "posts/04_bayesian_analysis/index.html",
    "href": "posts/04_bayesian_analysis/index.html",
    "title": "Mini-Project 4: Bayesian Analysis",
    "section": "",
    "text": "In this project we will be looking at the probability that Rafael Nadal wins a point on his serve against Novak Djokovic on the Clay surface. We will use 3, priors, 1 non-informative, 2 informative, and data from the players most recent match to create 3 different posterior distributions.\nlibrary(tidyverse)"
  },
  {
    "objectID": "posts/04_bayesian_analysis/index.html#non-informative-prior",
    "href": "posts/04_bayesian_analysis/index.html#non-informative-prior",
    "title": "Mini-Project 4: Bayesian Analysis",
    "section": "Non-informative Prior",
    "text": "Non-informative Prior\n\nps &lt;- seq(0, 1, length.out = 1000)\n\nnoninformative_alpha &lt;- 1\nnoninformative_beta &lt;- 1\n\nnoninformative_prior &lt;- dbeta(ps,\n                              noninformative_alpha, noninformative_beta)"
  },
  {
    "objectID": "posts/04_bayesian_analysis/index.html#first-informative-prior",
    "href": "posts/04_bayesian_analysis/index.html#first-informative-prior",
    "title": "Mini-Project 4: Bayesian Analysis",
    "section": "First Informative Prior",
    "text": "First Informative Prior\n\n## Nadal won 46/66 points \n## phat = 46/66 = 0.697\n## std error = 0.05657\n## 0.697 = alpha/(alpha + beta)\n## 0.697(a) + 0.697(b) = a\n## 0.697(b) = 0.303(a)\n## beta = (0.303(alpha))/0.697\n\ntarget_mean &lt;- 0.697\n\nalphas &lt;- seq(0.1, 60, length.out = 500)\nbetas &lt;- (0.303*alphas)/0.697\n\n\nparam_df &lt;- tibble(alphas, betas)\nparam_df &lt;- param_df |&gt; mutate(vars = \n                    (alphas*betas)/((alphas + betas)^2*(alphas + betas + 1)))\n\n\ntarget_var &lt;- 0.05657 ^ 2\n\nparam_df &lt;- param_df |&gt; mutate(dist_to_target = abs(vars - target_var))\n\nparam_df |&gt; filter(dist_to_target == min(dist_to_target))\n\n# A tibble: 1 × 4\n  alphas betas    vars dist_to_target\n   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;          &lt;dbl&gt;\n1   45.4  19.7 0.00320     0.00000378\n\nalpha &lt;- 45.4\n\nbeta &lt;- 19.7\n\ninformative_prior_1 &lt;- dbeta(ps, alpha, beta)\n\nFor this informative prior, I set out to find an alpha and beta that gave a target mean of 0.697, which matched Nadal’s win rate, and a variance of 0.05657^2 that matched the standard deviation. I do find the best fit for alpha and beta, I found the relationship between alpha and beta such that the mean is equal to 0.697, and then set the target variance to the square of the standard deviation. I then found the combination of alpha and beta that had the variance closest to the target. Our assumptions were the mean and std dev that were found with the past match data."
  },
  {
    "objectID": "posts/04_bayesian_analysis/index.html#second-informative-prior",
    "href": "posts/04_bayesian_analysis/index.html#second-informative-prior",
    "title": "Mini-Project 4: Bayesian Analysis",
    "section": "Second Informative Prior",
    "text": "Second Informative Prior\n\n## trying to get a mean of 0.75 and a probability\n## that lambda is less than 0.7 equal to 0.02\nalphas &lt;- seq(0.01, 300, length.out = 2000) \nbetas &lt;- (0.25*alphas)/0.75\n\ntarget_prob &lt;- 0.02\nprob_less_0.7 &lt;- pbeta(0.7, alphas, betas)\n\ntibble(alphas, betas, prob_less_0.7) |&gt;\n  mutate(close_to_target = abs(prob_less_0.7 - target_prob)) |&gt;\n  filter(close_to_target == min(close_to_target))\n\n# A tibble: 1 × 4\n  alphas betas prob_less_0.7 close_to_target\n   &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;           &lt;dbl&gt;\n1   252.  83.9        0.0200      0.00000517\n\nalpha2 &lt;- 251.828\n\nbeta2 &lt;- 83.943\n\ninformative_prior_2 &lt;- dbeta(ps, alpha2, beta2)\n\nFor this informative prior, I was looking for a mean of 0.75, and was trying to find a distribution that had nearly all of the data above 0.7. For this, I set a target that only 2% of the data would be below 0.7, and iterated through alpha and beta to find the the values where only 2% of data was below 0.7. Beta was also found to be proportional to alpha, where betas = (0.25*alphas)/0.75. Our assumptions were that the mean is 0.75, and that the majority of the data, 98 percent, is greater than 0.7."
  },
  {
    "objectID": "posts/04_bayesian_analysis/index.html#prior-distribution-plot",
    "href": "posts/04_bayesian_analysis/index.html#prior-distribution-plot",
    "title": "Mini-Project 4: Bayesian Analysis",
    "section": "Prior Distribution Plot",
    "text": "Prior Distribution Plot\n\nprior_plot &lt;- tibble(ps, informative_prior_2, informative_prior_1, noninformative_prior) |&gt;\n  pivot_longer(2:4, names_to = \"prior_type\", values_to = \"density\")\n\nggplot(data = prior_plot, aes(x = ps, y = density, colour = prior_type)) +\n  geom_line() +\n  scale_colour_viridis_d(end = 0.9) +\n  theme_minimal() +\n  labs(x = \"p\")"
  },
  {
    "objectID": "posts/04_bayesian_analysis/index.html#non-informative-posterior",
    "href": "posts/04_bayesian_analysis/index.html#non-informative-posterior",
    "title": "Mini-Project 4: Bayesian Analysis",
    "section": "Non-Informative Posterior",
    "text": "Non-Informative Posterior\n\n## 56 wins on 84 serves\n## alpha_post = yobs + alpha = 56 + 1 = 57\n## beta_post = n - yobs + beta = 84 - 56 + 1 = 29\nnon_inf_alpha_post &lt;- 57\nnon_inf_beta_post &lt;- 29\n\nnoninformative_post &lt;- dbeta(ps,\n                              non_inf_alpha_post, non_inf_beta_post)\n\nFor the non informative posterior, we use the equations alpha_post = yobs + alpha, and beta_post = n - yobs + beta, to get our variables for the posterior distribution."
  },
  {
    "objectID": "posts/04_bayesian_analysis/index.html#first-informative-posterior",
    "href": "posts/04_bayesian_analysis/index.html#first-informative-posterior",
    "title": "Mini-Project 4: Bayesian Analysis",
    "section": "First Informative Posterior",
    "text": "First Informative Posterior\n\n## 56 wins on 84 serves\n## alpha_post = yobs + alpha = 56 + 45.4 = 101.4\n## beta_post = n - yobs + beta = 84 - 56 + 19.7 = 47.7\n\ninf_alpha_post1 &lt;- 101.4\ninf_beta_post1 &lt;- 47.7\n\n\ninformative_post_1 &lt;- dbeta(ps,\n                              inf_alpha_post1, inf_beta_post1)"
  },
  {
    "objectID": "posts/04_bayesian_analysis/index.html#second-informative-posterior",
    "href": "posts/04_bayesian_analysis/index.html#second-informative-posterior",
    "title": "Mini-Project 4: Bayesian Analysis",
    "section": "Second Informative Posterior",
    "text": "Second Informative Posterior\n\n## 56 wins on 84 serves\n## alpha_post = yobs + alpha = 56 + 251.828 = 307.828\n## beta_post = n - yobs + beta = 84 - 56 + 83.943 = 111.943\n\ninf_alpha_post2 &lt;- 307.828\ninf_beta_post2 &lt;- 111.942\n\n\ninformative_post_2 &lt;- dbeta(ps,\n                              inf_alpha_post2, inf_beta_post2)\n\nFor the informative posteriors, we also use the equations alpha_post = yobs + alpha, and beta_post = n - yobs + beta, to get our variables for the posterior distributions. The beta and alpha are different for each of these and the non-informative prior."
  },
  {
    "objectID": "posts/04_bayesian_analysis/index.html#posterior-distribution-plot",
    "href": "posts/04_bayesian_analysis/index.html#posterior-distribution-plot",
    "title": "Mini-Project 4: Bayesian Analysis",
    "section": "Posterior Distribution Plot",
    "text": "Posterior Distribution Plot\n\npost_plot &lt;- tibble(ps, informative_post_2, informative_post_1, noninformative_post) |&gt;\n  pivot_longer(2:4, names_to = \"posterior_type\", values_to = \"density\")\n\nggplot(data = post_plot, aes(x = ps, y = density, colour = posterior_type)) +\n  geom_line() +\n  scale_colour_viridis_d(end = 0.9) +\n  theme_minimal() +\n  labs(x = \"p\")"
  },
  {
    "objectID": "posts/04_bayesian_analysis/index.html#posterior-means-and-90-credible-intervals",
    "href": "posts/04_bayesian_analysis/index.html#posterior-means-and-90-credible-intervals",
    "title": "Mini-Project 4: Bayesian Analysis",
    "section": "Posterior Means and 90% Credible Intervals",
    "text": "Posterior Means and 90% Credible Intervals\n\nnon_inf_mean &lt;- non_inf_alpha_post/(non_inf_alpha_post + non_inf_beta_post)\nnon_inf_upper &lt;- qbeta(0.95, non_inf_alpha_post, non_inf_beta_post)\nnon_inf_lower &lt;- qbeta(0.05, non_inf_alpha_post, non_inf_beta_post)\nnon_inf &lt;- tibble(lower = non_inf_lower,\n                  mean = non_inf_mean,\n                  upper = non_inf_upper)\nnon_inf\n\n# A tibble: 1 × 3\n  lower  mean upper\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 0.577 0.663 0.744\n\ninf_mean_1 &lt;- inf_alpha_post1/(inf_alpha_post1 + inf_beta_post1)\ninf_lower_1 &lt;- qbeta(0.05, inf_alpha_post1, inf_beta_post1)\ninf_upper_1 &lt;- qbeta(0.95, inf_alpha_post1, inf_beta_post1)\ninf_1 &lt;- tibble(lower = inf_lower_1,\n                mean = inf_mean_1,\n                upper = inf_upper_1)\ninf_1\n\n# A tibble: 1 × 3\n  lower  mean upper\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 0.616 0.680 0.741\n\ninf_mean_2 &lt;- inf_alpha_post2/(inf_alpha_post2 + inf_beta_post2)\ninf_lower_2 &lt;- qbeta(0.05, inf_alpha_post2, inf_beta_post2)\ninf_upper_2 &lt;- qbeta(0.95, inf_alpha_post2, inf_beta_post2)\ninf_2 &lt;- tibble(lower = inf_lower_2,\n                mean = inf_mean_2,\n                upper = inf_upper_2)\ninf_2\n\n# A tibble: 1 × 3\n  lower  mean upper\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 0.697 0.733 0.768\n\n\nFor each posterior mean, we use the equation, alpha/(alpha + beta) to find the mean. For the 90% credible intervals, we are looking for the bounds that give us the middle 90%, so using qbeta(p_prop, alpha, beta) gives us the desired bounds with the p_props of 0.05 and 0.95. Comparing the 3 posteriors, they all have slightly different ranges, with the non-informative having a mean of 0.663, and a 90% credible interval of (0.577, 0.744). This means for the non-informative there is a 90% probability that the proportion of clay court points Nadal wins against Djokovic is between 0.577 and 0.744. The 2 informative priors are different, with higher means and less variability. The first informative posterior, the mean is 0.68, and the 90% credible interval is (0.616, 0.741). The interval is much smaller than the non-informative. The second informative posterior has a slightly higher mean of 0.733, and a narrower 90% credible interval from (0.697, 0.768). The posteriors are all different because they have different data making the priors. The first prior has a wider credible interval and lower mean because there every p had the same probability. The informative posterior credible intervals were narrower because they had a prior distribution with means that were closer to the sample proportion. The 2 informative posteriors are different as they were designed with different data, and had different means and standard deviations based on their design parameters. The narrowest interval, the second informative posterior, is the narrowest because it was the most confident in its mean, and had the lowest standard deviation. I would select the first informative as the best. The first informative has a narrower 90% credible interval than the non-informative, and has a mean that is only slightly higher than our observed data. The second informative has a smaller credible interval, but the observed data is not in the credible interval. The beta and alpha from this prior were very large, so the data did not have as much of an impact on the posterior. The non-informative posterior is only based on the observed data so it has the same mean, but is less confident so has a much wider credible interval. The first informative uses data from a previous Nadal-Djokovic match so the posterior is a balance of the two matches and creates a distribution that is a good approximation for the proportion of points Nadal wins."
  },
  {
    "objectID": "posts/04_bayesian_analysis/index.html#conclusion",
    "href": "posts/04_bayesian_analysis/index.html#conclusion",
    "title": "Mini-Project 4: Bayesian Analysis",
    "section": "Conclusion",
    "text": "Conclusion\nIn this mini project we looked at the proportion of points that Nadal won on his serve in clay court matches against Djokovic. We made 3 prior distributions and then used the data from their match at the 2020 French Open to make 3 posterior distributions. We saw the different effect that the deviation of the prior distribution has on the posterior distribution. A smaller deviation means that alpha and beta values are larger, which means that our data has less of an effect on the posterior distribution. Our second prior has very large alpha and beta values, so even with the observed data our posterior distribution credible interval does not cover the observed sample proportion. For the non-informative prior the credible interval is significantly wider, and the standard deviation is large. This project shows that choosing an appropriate prior is very important for attaining a good posterior, as having a highly weighted prior that does not agree with the sample data will result in a posterior that does not include the observed data in the credible interval. Choosing a prior with a reasonable mean and medium standard deviation is probably most appropriate for finding a posterior that is appropriate for the scenario."
  },
  {
    "objectID": "posts/03_simulating_CI/index.html",
    "href": "posts/03_simulating_CI/index.html",
    "title": "Mini-Project 3: Simulation to Investigate Confidence Intervals",
    "section": "",
    "text": "n &lt;- 10   # sample size\np &lt;- 0.45  # population proportion\n\n\ngenerate_samp_prop &lt;- function(n, p) {\n  x &lt;- rbinom(1, n, p) # randomly generate number of successes for the sample\n\n  ## number of successes divided by sample size\n  phat &lt;- x / n\n\n  ## 95% confidence interval\n  lb &lt;- phat - 1.96 * sqrt(phat * (1 - phat) / n)\n  ub &lt;- phat + 1.96 * sqrt(phat * (1 - phat) / n)\n  \n  prop_df &lt;- tibble(phat, lb, ub)\n  return(prop_df)\n}\n\ngenerate_samp_prop(n = n, p = p)\n\n# A tibble: 1 × 3\n   phat      lb    ub\n  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1   0.1 -0.0859 0.286\n\n## how many ci's do we want?\nn_sim &lt;- 5000\n\nprop_ci_df &lt;- map(1:n_sim,\n    \\(i) generate_samp_prop(n = n, p = p)) |&gt;\n  bind_rows()\n\nprop_ci_df |&gt; mutate(coverage = if_else(p &gt; lb & p &lt; ub,\n                                   true = 1, \n                                   false = 0)) |&gt; \n        summarise(avg_interval_width = mean(ub - lb),\n                        avg_coverage_rate = mean(coverage))\n\n# A tibble: 1 × 2\n  avg_interval_width avg_coverage_rate\n               &lt;dbl&gt;             &lt;dbl&gt;\n1              0.583             0.882\n\n\n\nn &lt;- 10   # sample size\np &lt;- 0.2  # population proportion\n\n\ngenerate_samp_prop &lt;- function(n, p) {\n  x &lt;- rbinom(1, n, p) # randomly generate number of successes for the sample\n\n  ## number of successes divided by sample size\n  phat &lt;- x / n\n\n  ## 95% confidence interval\n  lb &lt;- phat - 1.96 * sqrt(phat * (1 - phat) / n)\n  ub &lt;- phat + 1.96 * sqrt(phat * (1 - phat) / n)\n  \n  prop_df &lt;- tibble(phat, lb, ub)\n  return(prop_df)\n}\n\ngenerate_samp_prop(n = n, p = p)\n\n# A tibble: 1 × 3\n   phat      lb    ub\n  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1   0.2 -0.0479 0.448\n\n## how many ci's do we want?\nn_sim &lt;- 5000\n\nprop_ci_df &lt;- map(1:n_sim,\n    \\(i) generate_samp_prop(n = n, p = p)) |&gt;\n  bind_rows()\n\nprop_ci_df |&gt; mutate(coverage = if_else(p &gt; lb & p &lt; ub,\n                                   true = 1, \n                                   false = 0)) |&gt; \n        summarise(avg_interval_width = mean(ub - lb),\n                        avg_coverage_rate = mean(coverage))\n\n# A tibble: 1 × 2\n  avg_interval_width avg_coverage_rate\n               &lt;dbl&gt;             &lt;dbl&gt;\n1              0.437             0.888"
  },
  {
    "objectID": "posts/03_simulating_CI/index.html#n-10",
    "href": "posts/03_simulating_CI/index.html#n-10",
    "title": "Mini-Project 3: Simulation to Investigate Confidence Intervals",
    "section": "",
    "text": "n &lt;- 10   # sample size\np &lt;- 0.45  # population proportion\n\n\ngenerate_samp_prop &lt;- function(n, p) {\n  x &lt;- rbinom(1, n, p) # randomly generate number of successes for the sample\n\n  ## number of successes divided by sample size\n  phat &lt;- x / n\n\n  ## 95% confidence interval\n  lb &lt;- phat - 1.96 * sqrt(phat * (1 - phat) / n)\n  ub &lt;- phat + 1.96 * sqrt(phat * (1 - phat) / n)\n  \n  prop_df &lt;- tibble(phat, lb, ub)\n  return(prop_df)\n}\n\ngenerate_samp_prop(n = n, p = p)\n\n# A tibble: 1 × 3\n   phat      lb    ub\n  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1   0.1 -0.0859 0.286\n\n## how many ci's do we want?\nn_sim &lt;- 5000\n\nprop_ci_df &lt;- map(1:n_sim,\n    \\(i) generate_samp_prop(n = n, p = p)) |&gt;\n  bind_rows()\n\nprop_ci_df |&gt; mutate(coverage = if_else(p &gt; lb & p &lt; ub,\n                                   true = 1, \n                                   false = 0)) |&gt; \n        summarise(avg_interval_width = mean(ub - lb),\n                        avg_coverage_rate = mean(coverage))\n\n# A tibble: 1 × 2\n  avg_interval_width avg_coverage_rate\n               &lt;dbl&gt;             &lt;dbl&gt;\n1              0.583             0.882\n\n\n\nn &lt;- 10   # sample size\np &lt;- 0.2  # population proportion\n\n\ngenerate_samp_prop &lt;- function(n, p) {\n  x &lt;- rbinom(1, n, p) # randomly generate number of successes for the sample\n\n  ## number of successes divided by sample size\n  phat &lt;- x / n\n\n  ## 95% confidence interval\n  lb &lt;- phat - 1.96 * sqrt(phat * (1 - phat) / n)\n  ub &lt;- phat + 1.96 * sqrt(phat * (1 - phat) / n)\n  \n  prop_df &lt;- tibble(phat, lb, ub)\n  return(prop_df)\n}\n\ngenerate_samp_prop(n = n, p = p)\n\n# A tibble: 1 × 3\n   phat      lb    ub\n  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1   0.2 -0.0479 0.448\n\n## how many ci's do we want?\nn_sim &lt;- 5000\n\nprop_ci_df &lt;- map(1:n_sim,\n    \\(i) generate_samp_prop(n = n, p = p)) |&gt;\n  bind_rows()\n\nprop_ci_df |&gt; mutate(coverage = if_else(p &gt; lb & p &lt; ub,\n                                   true = 1, \n                                   false = 0)) |&gt; \n        summarise(avg_interval_width = mean(ub - lb),\n                        avg_coverage_rate = mean(coverage))\n\n# A tibble: 1 × 2\n  avg_interval_width avg_coverage_rate\n               &lt;dbl&gt;             &lt;dbl&gt;\n1              0.437             0.888"
  },
  {
    "objectID": "posts/03_simulating_CI/index.html#n-60",
    "href": "posts/03_simulating_CI/index.html#n-60",
    "title": "Mini-Project 3: Simulation to Investigate Confidence Intervals",
    "section": "N = 60",
    "text": "N = 60\n\nn &lt;- 60   # sample size\np &lt;- 0.45  # population proportion\n\n\ngenerate_samp_prop &lt;- function(n, p) {\n  x &lt;- rbinom(1, n, p) # randomly generate number of successes for the sample\n\n  ## number of successes divided by sample size\n  phat &lt;- x / n\n\n  ## 95% confidence interval\n  lb &lt;- phat - 1.96 * sqrt(phat * (1 - phat) / n)\n  ub &lt;- phat + 1.96 * sqrt(phat * (1 - phat) / n)\n  \n  prop_df &lt;- tibble(phat, lb, ub)\n  return(prop_df)\n}\n\ngenerate_samp_prop(n = n, p = p)\n\n# A tibble: 1 × 3\n   phat    lb    ub\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 0.433 0.308 0.559\n\n## how many ci's do we want?\nn_sim &lt;- 5000\n\nprop_ci_df &lt;- map(1:n_sim,\n    \\(i) generate_samp_prop(n = n, p = p)) |&gt;\n  bind_rows()\n\nprop_ci_df |&gt; mutate(coverage = if_else(p &gt; lb & p &lt; ub,\n                                   true = 1, \n                                   false = 0)) |&gt; \n        summarise(avg_interval_width = mean(ub - lb),\n                        avg_coverage_rate = mean(coverage))\n\n# A tibble: 1 × 2\n  avg_interval_width avg_coverage_rate\n               &lt;dbl&gt;             &lt;dbl&gt;\n1              0.250             0.948\n\n\n\nn &lt;- 60   # sample size\np &lt;- 0.2  # population proportion\n\n\ngenerate_samp_prop &lt;- function(n, p) {\n  x &lt;- rbinom(1, n, p) # randomly generate number of successes for the sample\n\n  ## number of successes divided by sample size\n  phat &lt;- x / n\n\n  ## 95% confidence interval\n  lb &lt;- phat - 1.96 * sqrt(phat * (1 - phat) / n)\n  ub &lt;- phat + 1.96 * sqrt(phat * (1 - phat) / n)\n  \n  prop_df &lt;- tibble(phat, lb, ub)\n  return(prop_df)\n}\n\ngenerate_samp_prop(n = n, p = p)\n\n# A tibble: 1 × 3\n   phat    lb    ub\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1   0.3 0.184 0.416\n\n## how many ci's do we want?\nn_sim &lt;- 5000\n\nprop_ci_df &lt;- map(1:n_sim,\n    \\(i) generate_samp_prop(n = n, p = p)) |&gt;\n  bind_rows()\n\nprop_ci_df |&gt; mutate(coverage = if_else(p &gt; lb & p &lt; ub,\n                                   true = 1, \n                                   false = 0)) |&gt; \n        summarise(avg_interval_width = mean(ub - lb),\n                        avg_coverage_rate = mean(coverage))\n\n# A tibble: 1 × 2\n  avg_interval_width avg_coverage_rate\n               &lt;dbl&gt;             &lt;dbl&gt;\n1              0.199             0.924"
  },
  {
    "objectID": "posts/03_simulating_CI/index.html#n-200",
    "href": "posts/03_simulating_CI/index.html#n-200",
    "title": "Mini-Project 3: Simulation to Investigate Confidence Intervals",
    "section": "N = 200",
    "text": "N = 200\n\nn &lt;- 200   # sample size\np &lt;- 0.45  # population proportion\n\n\ngenerate_samp_prop &lt;- function(n, p) {\n  x &lt;- rbinom(1, n, p) # randomly generate number of successes for the sample\n\n  ## number of successes divided by sample size\n  phat &lt;- x / n\n\n  ## 95% confidence interval\n  lb &lt;- phat - 1.96 * sqrt(phat * (1 - phat) / n)\n  ub &lt;- phat + 1.96 * sqrt(phat * (1 - phat) / n)\n  \n  prop_df &lt;- tibble(phat, lb, ub)\n  return(prop_df)\n}\n\ngenerate_samp_prop(n = n, p = p)\n\n# A tibble: 1 × 3\n   phat    lb    ub\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 0.435 0.366 0.504\n\n## how many ci's do we want?\nn_sim &lt;- 5000\n\nprop_ci_df &lt;- map(1:n_sim,\n    \\(i) generate_samp_prop(n = n, p = p)) |&gt;\n  bind_rows()\n\nprop_ci_df |&gt; mutate(coverage = if_else(p &gt; lb & p &lt; ub,\n                                   true = 1, \n                                   false = 0)) |&gt; \n        summarise(avg_interval_width = mean(ub - lb),\n                        avg_coverage_rate = mean(coverage))\n\n# A tibble: 1 × 2\n  avg_interval_width avg_coverage_rate\n               &lt;dbl&gt;             &lt;dbl&gt;\n1              0.138             0.942\n\n\n\nn &lt;- 200   # sample size\np &lt;- 0.2  # population proportion\n\n\ngenerate_samp_prop &lt;- function(n, p) {\n  x &lt;- rbinom(1, n, p) # randomly generate number of successes for the sample\n\n  ## number of successes divided by sample size\n  phat &lt;- x / n\n\n  ## 95% confidence interval\n  lb &lt;- phat - 1.96 * sqrt(phat * (1 - phat) / n)\n  ub &lt;- phat + 1.96 * sqrt(phat * (1 - phat) / n)\n  \n  prop_df &lt;- tibble(phat, lb, ub)\n  return(prop_df)\n}\n\ngenerate_samp_prop(n = n, p = p)\n\n# A tibble: 1 × 3\n   phat    lb    ub\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 0.185 0.131 0.239\n\n## how many ci's do we want?\nn_sim &lt;- 5000\n\nprop_ci_df &lt;- map(1:n_sim,\n    \\(i) generate_samp_prop(n = n, p = p)) |&gt;\n  bind_rows()\n\nprop_ci_df |&gt; mutate(coverage = if_else(p &gt; lb & p &lt; ub,\n                                   true = 1, \n                                   false = 0)) |&gt; \n        summarise(avg_interval_width = mean(ub - lb),\n                        avg_coverage_rate = mean(coverage))\n\n# A tibble: 1 × 2\n  avg_interval_width avg_coverage_rate\n               &lt;dbl&gt;             &lt;dbl&gt;\n1              0.110             0.940"
  },
  {
    "objectID": "posts/03_simulating_CI/index.html#large-sample-assumptions",
    "href": "posts/03_simulating_CI/index.html#large-sample-assumptions",
    "title": "Mini-Project 3: Simulation to Investigate Confidence Intervals",
    "section": "Large Sample Assumptions",
    "text": "Large Sample Assumptions\nn = 10\np*n = 0.45x10 = 4.5 (1-p)n = 0.55x10 = 5.5 does not hold\np*n = 0.2x10 = 2 (1-p)n = 0.8x10 = 8 does not hold\nn = 60\np*n = 0.45x60 = 27 (1-p)n = 0.55x60 = 33 holds\np*n = 0.2x60 = 12 (1-p)n = 0.8x60 = 48 holds but not by a large margin\nn = 200\np*n = 0.45x200 = 90 (1-p)n = 0.55x200 = 110 holds\np*n = 0.2x200 = 40 (1-p)n = 0.8x200 = 160 holds\n\nTable of Results\n\n\n\n\nsn = 10\nmn = 60\nln = 200\n\n\n\n\n\\(p = 0.45\\)\nCoverage Rate\n0.876\n0.9496\n0.9534\n\n\n\\(p = 0.2\\)\nCoverage Rate\n0.887\n0.9246\n0.9446\n\n\n\n\n\n\n\n\n\n\\(p = 0.45\\)\nAverage Width\n0.5835463\n0.2496143\n0.1375476\n\n\n\\(p = 0.2\\)\nAverage Width\n0.4365931\n0.1997524\n0.110498\n\n\n\nStDev : sqrt(p(1-p)/n) Avg Width for 95% : 2*(1.96*stdev)\nFor our CI with a sample size of 10, we see that our average width is large, with a wide spread in the confidence interval. For both p levels, the large sample assumption does not hold, and our coverage rate is not 95%. If our p is equal to 0.45, we expect the width of our 95% CI to be 2* 1.96*sqrt((0.45(1-0.45))/10) = 0.6292. This is wider than our average width of 0.5835 for p = 0.45. This is why our coverage rate is less than 95%. For our p = 0.2, the large sample assumption also does not hold, so our average interval for a 95% confidence interval is wider than our sample average width. For our p = 0.2, our coverage rate of 88.7% is lower than the 95% confidence rate we would expect because our large sample size assumption does not hold. For our p = 0.45, the width of the CI is larger than 0.2, but the coverage rate is similarly different from 95%, because or large sample size assumption does not hold.\nFor a sample size of 60, we have a significantly smaller average width for our CI’s, for both p levels. The large sample size assumptions do hold, with n being large enough for both p values. If our p is equal to 0.45, we expect the width of our 95% CI to be 2* 1.96*sqrt((0.45(1-0.45))/60) = 0.2517. This is very close to our sample average width for n = 60, which explains the coverage rate of nearly 95%, with 94.96%. For p = 0.2, we expect the width of our 95% CI to be 2* 1.96* sqrt((0.2(1-0.2))/60) = 0.2024. This is slightly more than our sample average width which explains the slightly lower coverage rate of p = 0.2. The large sample assumption in this case holds but only by a small margin which explains the coverage rate that is less than the cases that have larger assumptions. The coverage rate is still very close to 95% with this mid level sample size.\nFor a sample size of 200, we have even smaller average widths for our CIs, and coverage rates very close to 95%. If our p is equal to 0.45, we expect the width of our 95% CI to be 2* 1.96*sqrt((0.45(1-0.45))/200) = 0.1379. This is very close to our sample value for average width, with a coverage rate of 95.34%, very close to 95%. For p = 0.2, we expect the width of our 95% CI to be 2* 1.96* sqrt((0.2(1-0.2))/200) = 0.11087. This value is also very close to our sample average width, leading to a coverage rate of 94.46%, again very close to 95%. We see as our sample size increases, our expected average width and sample average width begin to match better, leading to coverage rates that approach 95%. We can see for small sample sizes where the large sample size assumption does not hold, the coverage rate and expected average width are not nearly as close to the simulated values."
  }
]